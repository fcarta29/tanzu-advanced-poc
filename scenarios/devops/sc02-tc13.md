# SC02-TC13: Tanzu Kubernetes Cluster Ingress with NSX-ALB

For applications / workloads to be accessible outside of a Kubernetes cluster an object like the type Ingress needs to be created that either connects to or creates a Load Balancer that can route Layer 4 (L4) and/or Layer 7 (L7) network traffic. The NSX-ALB can be leveraged to provide either/both L4 and L7 routing.

---

## Test Case Summary

This test case focuses on how to setup and use NSX-ALB (AVI) and the Avi Kubernetes Operator (AKO) to create an Ingress and Layer 7 only Load Balancer for vSphere with Tanzu enabled and running on top NSX-T, which provides Layer 4.

>Note: The test procedure steps below are based on the following [Tanzu NSX-ALB Blog](https://yallavirtual.com/2020/11/23/tanzu-kubernetes-cluster-ingress-with-nsx-alb/)
---

## Prerequisites

* Completion of [SC01-TC01](../operator/sc01-tc01.md), [SC01-TC02](../operator/sc01-tc02.md), [SC02-TC01](sc02-tc01.md)
* vSphere Administrator console and user credentials

---

## Test Procedure

1. Using the vSphere Administrator console and credentials, login to the VC Web UI and navigate to **Menu > Workload Management**.

2. Select the **Namespaces** tab then the **${SC_NAMESPACE_01}** hyperlink.

3. Ensure `tkc01-small` is present in the ${SC_NAMESPACE_01} and reports healthy. If not deploy/redploy the `tkc01-small` cluster from the [SC02-TC01](sc02-tc01.md) test case.

4. Using the **DevOps Engineer** console and credentials, login to the `tkc01-small` Tanzu Kubernetes Cluster

    ```execute
    kubectl vsphere login --vsphere-username ${DEVOPS_USER_NAME} --server=https://${SC_API_VIP} --insecure-skip-tls-verify --tanzu-kubernetes-cluster-namespace ${SC_NAMESPACE_01} --tanzu-kubernetes-cluster-name tkc01-small</pre>
    ```

    Expected:
    <pre>Logged in successfully. <br> You have access to the following contexts: <br><b><i>${SC_API_VIP}</i></b><br>${SC_NAMESPACE_01}</pre>

5. Confirm you are logged into the *tkc01-small* Tanzu Kubernetes Cluster context

    ```execute
    kubectl config current-context
    ```

    Expected:
    <pre>tkc01-small</pre>

6. Steps 6-15 will be performed from the AVI Controller Management UI. First we need to create an IPAM and DNS Profile to automate creating DNS entries and IP address assignments when an Ingress is created. To create a DNS Profile **Templates >> Profiles >> IPAM/DNS Profiles >> Create DNS Profile**. To create the IPAM Profile **Templates > Profiles > IPAM/DNS Profiles > Create IPAM Profile**.

7. Add IPAM and DNS Profiles to Default Cloud, which is a vCenter Cloud. Select the profiles created above to the appropriate drop downs at **Infrastructure > Clouds > Default Cloud**.

8. While on Default Cloud configuration enable three things, **DHCP**, **Prefer Static Routes vs Directly Connected Network**, and **Use Static Routes for Network Resolution of VIP**.

9. In Default Cloud configuration to the **Network** tab and view/select the **Management Network** dropdown to the manage VDS dPG, or management NSX-T Overlay.

10. Each Tanzu Kubernetes Cluster will require its own SE Group. Navigate to **Infrastructure > Service Engine Group > CREATE**, supply a Service Engine name and leave the rest to defaults.

11. Cofigure VIP Network and default routes by going to **Infrastructure > Networks** add an IP Pool for the Data Network which will be used for the VIPs.

12. Configure default route by going to **Infrastructure > Routing** and create/configure the default gateway.

13. (Optional) You can manually configure the Service Engines (SE) - see `Tanzu NSX-ALB Blog` link above but for this test case we will allow the automatic creating of the SEs.

14. To setup DNS delegation first create a DNS Virtual Service (DNS-VS) **Application > Virtual Service > CREATE VIRTUAL SERVICE**

15. Enable the DNS-VS by going to **Administration > Settings > DNS > Service** and selecting the DNS-VS created in the step above.

16. On the main DNS Server add a subdomain entry to point to the NSX-ALB DNS-VS for DNS delegation to work.

17. Going back to the **DevOps Engineer** console create the `avi-system` namespace for the Avi Kubernetes Operator (AKO) installation.

    ```execute
    kubectl create ns avi-system
    ```

    Expected:
    <pre>namespace/avi-system created</pre>

18. Add AKO repo bits to helm (TODO[fcarta] - update with tanzu AKO link?)

    ```execute
    helm repo add ako https://avinetworks.github.io/avi-helm-charts/charts/stable/ako 
    ```

    Expected:
    <pre></pre>

19. Copy down the AKO values file and edit values. (TODO[fcarta] - add details here or provide values.yaml file in lab)

    ```execute
    curl -JOL https://raw.githubusercontent.com/avinetworks/avi-helm-charts/master/charts/stable/ako/values.yaml 
    ```

    Expected:
    <pre></pre>

20. Deploy AKO using helm

    ```execute
    helm install ako/ako --generate-name --version 1.2.1 -f values.yaml --namespace=avi-system
    ```

    Expected:
    <pre></pre>

21. Poll AKO pod status and wait for it to report a `Running` state.

    ```execute
     kubectl get pods -n avi-system -w
    ```

    Expected:
    <pre>
    NAME     READY  STATUS   RESTARTS   AGE
    ako-0    1/1    Running     0       1h
    </pre>

22. Verify in AVI Controller Management UI that that AKO create static routes under **Infrastructure Routing**.

23. Next steps are to validate automatic Load Balancer creation by deploying a sample application. Create an application namespace `yelb`.

    ```execute
    kubectl create ns yelb
    ```

    Expected:
    <pre></pre>

24. Deploy the `yelb` test application.

    ```execute
    kubectl apply -f https://raw.githubusercontent.com/aidrees/yelb/main/yelb-no-lb.yaml -n yelb
    ```
    
    Expected:
    <pre></pre>

25. Apply the `yelb` Ingress by downloading the yaml file, editing the **host** field with a DNS delegated name and save, and then applying that edited yaml file.

    ```execute
    curl -JOL https://raw.githubusercontent.com/aidrees/yelb/main/yelb-ingress.yaml
    ```

    ```execute
    kubectl apply -f yelb-ingress.yaml -n yelb
    ```
    
    Expected:
    <pre></pre>

26. View the newly created Ingress

    ```execute
    kubectl get ingress -n yelb
    ```
    
    Expected:
    <pre>
    NAME           HOSTS                    ADDRESS          PORTS   AGE
    yelb-ingress   yelb.ali-avi.vmwdxb.com   192.168.28.101   80     14m
    </pre>

27. TODO[fcarta] - complete validation steps and add example with Service of Type LoadBalancer



---

## Additional Tanzu related Load Balancing Information

* [Configuring vSphere Networking and NSX Advanced Load Balancer for vSphere with Tanzu](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-8908FAD7-9343-491B-9F6B-45FA8893C8CC.html)
* [NSX Advanced Load Balancer Components](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-A247F5F2-AC7E-48E7-B615-F8D361C7292A.html?hWord=N4IghgNiBc4NYHsQF8g)
* [Install VMware NSX Advanced Load Balancer on a vSphere Distributed Switch](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.3/vmware-tanzu-kubernetes-grid-13/GUID-mgmt-clusters-install-nsx-adv-lb.html)

---

## Status Pass/Fail

* [  ] Pass
* [  ] Fail

Return to [Test Cases Inventory](../../README.md#Test-Cases-Inventory)
