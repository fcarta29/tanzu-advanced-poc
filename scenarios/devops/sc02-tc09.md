# SC02-TC08: CaaS control plane shall provide support for canary releases - app with 2 versions

---

## Prerequisites

* DevOps Engineer console and user credentials
* TSM enabled Kubernetes Cluster

  1. Install TSM extension in TMC. From the TMC portal click on **Administration** in the left menu and then select the **Integrations** tab on the main window pane. If not already enabled click to **ENABLE** Tanzu Service Mesh.

  2. From TMC, install TSM in a cluster and onboard it. 

---

## Test Procedure


With Istio, traffic routing and replica deployment are two completely independent functions. The number of pods implementing services are free to scale up and down based on traffic load, completely orthogonal to the control of version traffic routing. This makes managing a canary version in the presence of autoscaling a much simpler problem. Autoscalers may, in fact, respond to load variations resulting from traffic routing changes, but they are nevertheless functioning independently and no differently than when loads change for other reasons.

Istio’s routing rules also provide other important advantages; you can easily control fine-grained traffic percentages (e.g., route 1% of traffic without requiring 100 pods) and you can control traffic using other criteria (e.g., route traffic for specific users to the canary version). To illustrate, let’s look at deploying the helloworld service and see how simple the problem becomes.

This sample includes two versions of a simple helloworld service that returns its version and instance (hostname) when called.It can be used as a test service when experimenting with version routing.


1. Change context to the kubernetes cluster where Istio Service Mesh(TSM) is installed

    ```execute
    kubectl vsphere login --server=${SC_API_VIP} --vsphere-username ${SC_USER_NAME} --insecure-skip-tls-verify --tanzu-kubernetes-cluster-namespace ${TKC_NAMESPACE} --tanzu-kubernetes-cluster-name ${TKC_NAME}
    ```

    Change kubernetes context
    ```execute
    kubectl config use-context ${TKC_NAME}
    ```    

2. We begin by defining the helloworld Service, just like any other Kubernetes service, To run different versions of the helloworld service, use the following command:

    ```execute
    kubectl apply -f helloworld.yaml
    ```

    Alternatively, you can run just one version at a time by first defining the service:

    ```
    kubectl apply -f helloworld.yaml -l app=helloworld
    ```
    and then deploying version v1, v2, or both:

    ```
    kubectl apply -f helloworld.yaml -l version=v1
    kubectl apply -f helloworld.yaml -l version=v2
    ```

    Note that this is exactly the same way we would do a canary deployment using plain Kubernetes, but in that case we would need to adjust the number of replicas of each Deployment to control the distribution of traffic. For example, to send 10% of the traffic to the canary version (v2), the replicas for v1 and v2 could be set to 9 and 1, respectively.


3. Define traffic routing - since we are going to deploy the service in an Istio enabled cluster, all we need to do is set a routing rule to control the traffic distribution. For example if we want to send 10% of the traffic to the canary, we could use kubectl to set a routing rule using virtual service and destination rule-

    ```execute
    kubectl apply -f virtualservice-90-10.yaml
    ```

After setting this rule, Istio will ensure that only one tenth of the requests will be sent to the canary version, regardless of how many replicas of each version are running.

4. Export LOADBALANCER_EXTERNAL_IP by setting istio-ingressgateway LoadBalancer external IP 

    ```execute
    export LOADBALANCER_EXTERNAL_IP=`kubectl get svc istio-ingressgateway -n istio-system  | awk 'NR == 2 {print $4}'`
    ```

5. Invoke service multiple times and verify traffic routing

    ```execute
    curl  -H "Host: helloworld.avi-ns1.poc6349.wwtatc.lab" http://$LOADBALANCER_EXTERNAL_IP/hello 
    ```


### Autoscaling the deployments

Because we don’t need to maintain replica ratios anymore, we can safely add Kubernetes horizontal pod autoscalers to manage the replicas for both version Deployments:

1. Enable autoscaling on both versions of the service:

    ```execute
    kubectl autoscale deployment helloworld-v1 --cpu-percent=50 --min=1 --max=10
    kubectl autoscale deployment helloworld-v2 --cpu-percent=50 --min=1 --max=10
    ```

2. Check autoscaling 

    ```execute
    kubectl get hpa
    ```

    NAME           REFERENCE                 TARGET  CURRENT  MINPODS  MAXPODS  AGE
    Helloworld-v1  Deployment/helloworld-v1  50%     47%      1        10       17s
    Helloworld-v2  Deployment/helloworld-v2  50%     40%      1        10       15s


3. Now generate some load on the helloworld service, we would notice that when scaling begins, the v1 autoscaler will scale up its replicas significantly higher than the v2 autoscaler replicas because v1 pods are handling 90% of the load.

    ```execute t2
    ./loadgen.sh
    ```


4. Check pods 

    ```execute
    kubectl get pods | grep helloworld
    ```

    helloworld-v1-3523621687-3q5wh   0/2       Pending   0          15m
    helloworld-v1-3523621687-73642   2/2       Running   0          11m
    helloworld-v1-3523621687-7hs31   2/2       Running   0          19m
    helloworld-v1-3523621687-dt7n7   2/2       Running   0          50m
    helloworld-v1-3523621687-gdhq9   2/2       Running   0          11m
    helloworld-v1-3523621687-jxs4t   0/2       Pending   0          15m
    helloworld-v1-3523621687-l8rjn   2/2       Running   0          19m
    helloworld-v1-3523621687-wwddw   2/2       Running   0          15m
    helloworld-v1-3523621687-xlt26   0/2       Pending   0          19m
    helloworld-v2-4095161145-963wt   2/2       Running   0          50m

5. Change the routing rule to send 50% of the traffic to v2. 


    ```execute
    kubectl apply -f virtualservice-50-50.yaml
    ```


6. After a short delay, notice that the v1 autoscaler will scale down the replicas of v1 while the v2 autoscaler will perform a corresponding scale up.

    ```execute
    kubectl get pods | grep helloworld
    ```

    helloworld-v1-3523621687-73642   2/2       Running   0          35m
    helloworld-v1-3523621687-7hs31   2/2       Running   0          43m
    helloworld-v1-3523621687-dt7n7   2/2       Running   0          1h
    helloworld-v1-3523621687-gdhq9   2/2       Running   0          35m
    helloworld-v1-3523621687-l8rjn   2/2       Running   0          43m
    helloworld-v2-4095161145-57537   0/2       Pending   0          21m
    helloworld-v2-4095161145-9322m   2/2       Running   0          21m
    helloworld-v2-4095161145-963wt   2/2       Running   0          1h
    helloworld-v2-4095161145-c3dpj   0/2       Pending   0          21m
    helloworld-v2-4095161145-t2ccm   0/2       Pending   0          17m
    helloworld-v2-4095161145-v3v9n   0/2       Pending   0          13m

7. Now stop generating load


8. Check replicas - both versions will eventually scale down to their minimum (1), regardless of what routing rule we set.

    ```execute
    kubectl get pods | grep helloworld
    ```

    helloworld-v1-3523621687-dt7n7   2/2       Running   0          1h
    helloworld-v2-4095161145-963wt   2/2       Running   0          1h

9. cleanup 

    ```execute
    kubectl delete -f helloworld.yaml
    kubectl delete -f virtualservice-90-10.yaml
    ```


## Status Pass/Fail

* [  ] Pass
* [  ] Fail

