# SC01-TC01: Enable a vSphere Cluster for Kubernetes Workloads

When vSphere with Tanzu is enabled on a vSphere cluster, it creates a Kubernetes control plane inside the hypervisor layer. This layer contains specific objects that enable the capability to run Kubernetes workloads within ESXi. A cluster that is enabled for vSphere with Tanzu is called a Supervisor Cluster (SC). It runs on top of an SDDC layer that consists of ESXi for compute, NSX-T Data Center for networking, and vSAN or another shared storage solution.

---

## Test Case Summary

The test case procedure demonstrates enabling a compatible vSphere 7 cluster as a SC. After enabling the SC, the procedure verifies that the SC Web UI is available for downloading client tools and the API processes authentication, authorization, and Kubernetes resource requests.

---

## Prerequisites

* Completion of [Prerequisites](prerequisites.md)
* Completion of [Enable Workload Management](enable-workload-management.md)

---

## Test Procedure

1. If not already done, enable Workload Management [Enable Workload Management](enable-workload-management.md)

2. Authenticate to SC

    ```execute
    kubectl-vsphere login --vsphere-username ${SC_ADMIN_NAME}@vsphere.local --server=https://${SC_API_VIP}--insecure-skip-tls-verify
    ```

    Expected:
    <pre>Logged in successfully.<br>You have access to the following contexts:<br><b><i>${SC_API_VIP}</i></b></pre>

3. Set the context for the SC with the command.

    <pre>kubectl config use-context <b><i>${SC_API_VIP}</i></b></pre>

    Expected:
    <pre><b><i>${SC_API_VIP}</i></b></pre>

4. Verify status of SC, Kubernetes master and URL with the command 

    ```execute
    kubectl cluster-info
    ```

    Expected:
    <pre>Kubernetes master is running at https://<b><i>${SC_API_VIP}</i>:6443<br>...</pre>

5. Verify all SC control-plane nodes are available API server endpoints.

    ```execute
    kubectl get endpoints
    ```

    Expected:
    <pre>NAME         ENDPOINTS<br>kubernetes   <i>Ctrl_Plane_Node_01_IP</i>:6443,<i>Ctrl_Plane_Node_02_IP</i>:6443,<i>Ctrl_Plane_Node_03_IP</i>:6443</pre>

6. Verify status, role, and version for all control-plane nodes and worker nodes.

    ```execute
    kubectl get nodes
    ```

    Expected:
    <pre>NAME                        STATUS   ROLES   VERSION<br><i>Ctrl_Plane_Node_01_UUID</i>     Ready    master  v1.16.7~<br><i>Ctrl_Plane_Node_03_UUID</i>     Ready    master  v1.16.7~<br><i>Ctrl_Plane_Node_02_UUID</i>     Ready    master  v1.16.7~<br><i>ESXi_Host_01_FQDN/IP</i>        Ready    agent   v1.16.7~<br><i>ESXi_Host_02_FQDN/IP</i>        Ready    agent   v1.16.7~<br><i>ESXi_Host_03_FQDN/IP</i>        Ready    agent   v1.16.7~<br>â€¦</pre>Note: VMware does not update this document with every TKG release. The versions reported above represent sample output.|

7. Run to verify all SC pods report either `Running` or `Completed`.

    ```execute
    kubectl get pods -A
    ```

    Expected:
    <pre>All pods report `STATUS` as either **`Running`** or **`Completed`**</pre>

---

## Status Pass/Fail

* [  ] Pass
* [  ] Fail

Return to [Test Cases Inventory](../../README.md###Test-Cases-Inventory)
